Why using max subtraction in softmax?
This is done for stability reasons, here's the [blog](https://nolanbconaway.github.io/blog/2017/softmax-numpy)

Notes from this [forum in Coursera(https://www.coursera.org/learn/nlp-sequence-models/discussions/weeks/1/threads/dsNnKAjPEeimBg7ZCr_dBg)] with defining `def rnn_backward(da, caches)`, image can be seen [here](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0psMGgjWEei_ZQ6W1G11dA_19bfc02a092b703c0aa904685e71c9f1_1.png?expiry=1525564800000&hmac=-UCnDlNbUK9__5BSo7LJDX2JdzU9gnECbibZS1G1irE)

[Hand-written derivation for LSTM backward pass](https://www.coursera.org/learn/nlp-sequence-models/discussions/weeks/1/threads/Izwswh-eEeiQEBKP_jhcNg)

[Errors (and Corrections) in description of "3.2 LSTM backward pass" and expected output of 3.3](https://www.coursera.org/learn/nlp-sequence-models/programming/xxuVc/building-a-recurrent-neural-network-step-by-step/discussions/threads/6wZbVAidEei4jA4iWxMvzg)

[bigger parentheses in equations](https://tex.stackexchange.com/questions/78736/bigger-parentheses-in-equations)

[numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)
